{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "# import pickle # use for saving and loading variables calculated by function step2_indexing,\n",
    "#               # so that in the next time there will be no need to run step2_indexing again\n",
    "\n",
    "\n",
    "def alphabet_validate(token):\n",
    "    for i in token:\n",
    "        if not ( i >= u'\\u0061' and i <= u'\\u007A'):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def queryToTokens(queryString,sp=spacy.load('en_core_web_sm')):\n",
    "    tokens = []\n",
    "    sentence = sp(queryString)\n",
    "    for token in sentence:\n",
    "        if not token.is_stop and not token.is_punct and not token.is_space and token.text[:7]!='http://':\n",
    "            if not alphabet_validate(token.lower_):\n",
    "                result = re.findall(r'[a-z]+', token.lower_)\n",
    "                for i in result:\n",
    "                    if len(i) > 2:\n",
    "                        tokens.append(i.lower())\n",
    "                continue\n",
    "            if len(token.text) > 2:\n",
    "                tokens.append(token.lemma_.lower())\n",
    "    return tokens\n",
    "\n",
    "def step1_preprocessing():\n",
    "    file = open(\"./Trec_microblog11.txt\", mode='r', encoding='UTF-8-sig')\n",
    "    token_array = []\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    line = file.readline()\n",
    "    count = 0 \n",
    "    while line:\n",
    "        count+=1\n",
    "        if (count%1000 == 0):\n",
    "            print(\"Processing line {}\".format(str(count)),end=\"\\r\")\n",
    "        tokens = queryToTokens(line,sp)\n",
    "        tokens.insert(0, line.split()[0])\n",
    "        token_array.append(tokens)\n",
    "        line = file.readline()\n",
    "    file.close()\n",
    "    print(\"\\nFinished preprocessing!\")\n",
    "    return(token_array)\n",
    "\n",
    "\n",
    "\n",
    "def step2_indexing(token_array):\n",
    "    corpus_size = len(token_array)\n",
    "    # build vocabulary list\n",
    "#     start = time.time()\n",
    "#     vocabulary = []\n",
    "#     for doc in token_array:\n",
    "#         for token in doc[1:]:\n",
    "#             if not token in vocabulary:\n",
    "#                 vocabulary.append(token)      \n",
    "#     print(\"method1:%ss\"%(time.time()-start)) #91.39s\n",
    "    \n",
    "    \n",
    "#     start = time.time()\n",
    "    vocabulary=set()\n",
    "    for doc in token_array:\n",
    "        for token in doc[1:]:\n",
    "            vocabulary.add(token)\n",
    "    vocabulary=list(vocabulary)\n",
    "#     print(\"method2:%ss\"%(time.time()-start)) #0.1s\n",
    "\n",
    "    # initialize inverted index\n",
    "    inverted_index = []\n",
    "    for term in vocabulary:\n",
    "        entry = []\n",
    "        entry.append([term, 0])\n",
    "        entry.append([])\n",
    "        inverted_index.append(entry)\n",
    "\n",
    "    # build inverted index\n",
    "    doc_count = 0\n",
    "    max_tf = 0\n",
    "    for doc in token_array:\n",
    "        docno = doc[0] # document number\n",
    "        token_list = doc[1:]\n",
    "        # calculate the number of occurences of each token in a document by using dictionary\n",
    "        dictionary = {}\n",
    "        for key in token_list:\n",
    "            dictionary[key] = dictionary.get(key, 0) + 1\n",
    "\n",
    "        # update inverted index by the information of the document\n",
    "        for key in dictionary:\n",
    "            tf = dictionary[key]\n",
    "            doc_num_tf_pair = [docno, tf]\n",
    "            key_index = vocabulary.index(key)\n",
    "            inverted_index[key_index][1].append(doc_num_tf_pair)\n",
    "            # update df\n",
    "            inverted_index[key_index][0][1] += 1\n",
    "            # update max tf\n",
    "            if tf > max_tf:\n",
    "                max_tf = tf\n",
    "\n",
    "        doc_count += 1\n",
    "        if doc_count % 1000 == 0:\n",
    "            print(\"Finished processing {} documents\".format(doc_count),end=\"\\r\")\n",
    "            \n",
    "    print(\"Finished processing {} documents\".format(doc_count),end=\"\\r\")\n",
    "    print()\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    print(\"Finished building inverted index,\")\n",
    "    print(\"The size of vocabulary is:\", vocabulary_size)\n",
    "\n",
    "    return inverted_index, vocabulary, max_tf\n",
    "\n",
    "\n",
    "\n",
    "def step3_retrieval_and_ranking(token_array, inverted_index, vocabulary, max_tf, doc_docno_search_list, query_token_list):\n",
    "    # find the limited set of documents that contain at least one of the query words\n",
    "    related_document_number_list = []\n",
    "    for query_token in query_token_list:\n",
    "        query_token_index = vocabulary.index(query_token)\n",
    "        doc_num_tf_pair_list = inverted_index[query_token_index][1]\n",
    "        for doc_num_tf_pair in doc_num_tf_pair_list:\n",
    "            doc_num = doc_num_tf_pair[0]\n",
    "            if not doc_num in related_document_number_list:\n",
    "                related_document_number_list.append(doc_num)\n",
    "\n",
    "    # build limited corpus\n",
    "    related_document_location_list = []\n",
    "    for docno in related_document_number_list:\n",
    "        related_document_location_list.append(doc_docno_search_list.index(docno))\n",
    "\n",
    "    limited_corpus = []\n",
    "    for location in related_document_location_list:\n",
    "        limited_corpus.append(token_array[location])\n",
    "\n",
    "    # build new vocabulary list\n",
    "    new_vocabulary = []\n",
    "    for doc in limited_corpus:\n",
    "        for token in doc[1:]:\n",
    "            if not token in new_vocabulary:\n",
    "                new_vocabulary.append(token)\n",
    "\n",
    "    # calculate idf list\n",
    "    corpus_size = len(token_array)\n",
    "    new_vocabulary_idf_list = []\n",
    "    for term in new_vocabulary:\n",
    "        term_index = vocabulary.index(term)\n",
    "        df = inverted_index[term_index][0][1]\n",
    "        idf = math.log(corpus_size / df, 2)\n",
    "        new_vocabulary_idf_list.append(idf)\n",
    "\n",
    "    # build document-docno search list for the limited corpus\n",
    "    limited_corpus_doc_docno_search_list = []\n",
    "    for doc in limited_corpus:\n",
    "        limited_corpus_doc_docno_search_list.append(doc[0])\n",
    "\n",
    "    # build tf-idf matrix\n",
    "    limited_corpus_size = len(limited_corpus)\n",
    "    new_vocabulary_size = len(new_vocabulary)\n",
    "    tf_idf_matrix = np.zeros((limited_corpus_size, new_vocabulary_size), dtype=np.float)\n",
    "\n",
    "    for query_token in query_token_list:\n",
    "        query_token_index = vocabulary.index(query_token)\n",
    "        doc_num_tf_pair_list = inverted_index[query_token_index][1]\n",
    "        column_num = new_vocabulary.index(query_token)\n",
    "        idf = new_vocabulary_idf_list[column_num]\n",
    "        for doc_num_tf_pair in doc_num_tf_pair_list:\n",
    "            docno = doc_num_tf_pair[0]\n",
    "            tf = doc_num_tf_pair[1]\n",
    "            row_num = limited_corpus_doc_docno_search_list.index(docno)\n",
    "            # normalize term frequency (tf) across the entire corpus\n",
    "            # and here we put the process of building term frequency matrix and\n",
    "            # tf-idf matrix together, to save the time for processing each query\n",
    "            tf_idf_matrix[row_num][column_num] = (tf / max_tf) * idf\n",
    "\n",
    "    # # multiply the tf scores by the idf values of each term,\n",
    "    # # obtaining the following tf-idf matrix\n",
    "    # tf_idf_matrix = np.zeros((limited_corpus_size, new_vocabulary_size), dtype=np.int)\n",
    "    # for column_num in range(new_vocabulary_size):\n",
    "    #     idf = new_vocabulary_idf_list[column_num]\n",
    "    #     for row_num in range(limited_corpus_size):\n",
    "    #         tf = tf_matrix[row_num][column_num]\n",
    "    #         tf_idf_matrix[row_num][column_num] = tf * idf\n",
    "\n",
    "\n",
    "    # calculate the tf-idf vector for the query\n",
    "    query_tf_idf_vector = np.zeros((1, new_vocabulary_size), dtype=np.float)\n",
    "    # first calculate the number of occurences of each token in the query by using dictionary\n",
    "    dict = {}\n",
    "    for key in query_token_list:\n",
    "        dict[key] = dict.get(key, 0) + 1\n",
    "    # find the maximum token frequency of the query\n",
    "    query_max_tf = 0\n",
    "    for key in dict:\n",
    "        tf = dict[key]\n",
    "        if tf > query_max_tf:\n",
    "            query_max_tf = tf\n",
    "    # calculate the query's tf-idf vector\n",
    "    for key in dict:\n",
    "        tf = dict[key]\n",
    "        key_index = new_vocabulary.index(key)\n",
    "        idf = new_vocabulary_idf_list[key_index]\n",
    "        # a modified tf-idf weighting scheme w_iq = (0.5 + 0.5 tf_iq)âˆ™idf_i\n",
    "        query_tf_idf_vector[0][key_index] = (0.5 + 0.5 * (tf / query_max_tf)) * idf\n",
    "        # # traditional tf-idf weighting scheme\n",
    "        # query_tf_idf_vector[0][key_index] = (tf / query_max_tf) * idf\n",
    "\n",
    "    # compute the similarity scores between a query and each document using cosine\n",
    "    # save the result into a dictionary\n",
    "    similarity_dictionary = {}\n",
    "    query_tf_idf_vector_length = np.linalg.norm(query_tf_idf_vector)\n",
    "    query_tf_idf_vector = query_tf_idf_vector.flatten()\n",
    "    for row_num in range(limited_corpus_size):\n",
    "        docno = limited_corpus_doc_docno_search_list[row_num]\n",
    "        document_tf_idf_vector = tf_idf_matrix[row_num]\n",
    "        cosSim = sum(document_tf_idf_vector * query_tf_idf_vector) / (np.linalg.norm(document_tf_idf_vector) * query_tf_idf_vector_length)\n",
    "        similarity_dictionary[docno] = cosSim\n",
    "\n",
    "    return similarity_dictionary\n",
    "\n",
    "def loadQueries(path:\"String of test query path\")->\"Dict in the format {queryNum('B001'):queryString('BBC World Service staff cuts')}\":\n",
    "    title = []\n",
    "    num = []\n",
    "    with open(path, \"r\") as f:  \n",
    "        for line in f:        \n",
    "            if line[:7] == \"<title>\":\n",
    "                title.append(line[8:-10])\n",
    "            elif line[:5] == \"<num>\": \n",
    "                num.append(line[15:-8])\n",
    "    return dict(zip(num,title))\n",
    "\n",
    "def loadResult(token_array, inverted_index,vocabulary, max_tf, doc_docno_search_list, queries)->\"Dict{queryNum:Dict_DesendingScoreOrder{tweetid:score}}\":\n",
    "    returnValue = {}\n",
    "    timeRecording=[]\n",
    "    for num, query in queries.items():\n",
    "        print(\"processing query {} of {}\".format((int)(num[2:].lstrip('0')),len(queries)),end=\"\\r\")\n",
    "        #####preprocess query\n",
    "        sp = spacy.load('en_core_web_sm')\n",
    "        query_token_list = queryToTokens(query, sp)\n",
    "        start=time.time()\n",
    "        result = step3_retrieval_and_ranking(token_array, inverted_index, vocabulary, max_tf, doc_docno_search_list, query_token_list)\n",
    "        timeRecording.append(time.time()-start)\n",
    "        sortedResult=dict(sorted(result.items(), key=lambda result:result[1], reverse=True))       \n",
    "        returnValue[(int)(num[2:].lstrip('0'))] = dict(zip(list(sortedResult.keys())[:1000],sortedResult.values()))\n",
    "    print()\n",
    "    avgTime=round(sum(timeRecording)/len(timeRecording),2)\n",
    "    print(\"Average time for each query processing: %ss\"%(avgTime))\n",
    "    return returnValue\n",
    "    \n",
    "    \n",
    "\n",
    "def loadExpectedResult(expectedPath:\"path of Trec_microblog11-qrels.txt\")->\"Dict{QueryNum:Set{all relevant ids}}\":\n",
    "    title = []\n",
    "    num = []\n",
    "    lastQueryNum = 0\n",
    "    returnValue = {}\n",
    "    with open(expectedPath, \"r\") as ef:  \n",
    "        for line in ef: \n",
    "            lst = line[:-1].split(\"\\t\") #[queryNum, 0, id, relevant]\n",
    "            currentQueryNum = int(lst[0])\n",
    "            if (currentQueryNum != lastQueryNum):\n",
    "                returnValue[currentQueryNum] = set() # add an empty set in the dict\n",
    "            lastQueryNum = currentQueryNum\n",
    "            if lst[3] == '1': # if relevant          \n",
    "                returnValue[currentQueryNum].add(lst[2]) # add id to the set\n",
    "                \n",
    "    return returnValue\n",
    "\n",
    "\n",
    "def saveResults(result,queries,outputPath,debug=False):\n",
    "    '''\n",
    "    step 4\n",
    "    '''\n",
    "    if (debug==True):\n",
    "        outputPath = 'output/Result_{}.txt'.format(str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())))\n",
    "    with open(outputPath, 'w+') as f:\n",
    "        f.write('topic_id Q0 docno rank score tag\\n')\n",
    "        for num, idScore in result.items():\n",
    "            rank = 1\n",
    "            for tweetId,score in idScore.items():\n",
    "                f.write(\"{} {} {} {} {} {}\".format(num, 'Q0', tweetId, rank, score,'myRun\\n'))\n",
    "                rank+=1\n",
    "    return\n",
    "\n",
    "def computeMAP(result:\"Dict{queryNum:tweetid list in decending order}\",expect:\"Dict{QueryNum:Set{all relevant ids}}\",\n",
    "               first10:\"set True if compute AP only for first 10\"=False) -> \"MAP or P@10\":\n",
    "    '''\n",
    "    step 5\n",
    "    '''\n",
    "    APs=[]\n",
    "    for queryNum,resultList in result.items():\n",
    "\n",
    "        expectForTheQuery = expect[queryNum].keys()\n",
    "        ids = resultList\n",
    "        relevant = 1\n",
    "        relevantDocument = []\n",
    "        \n",
    "        for i in range(1, min(11, len(ids)+1) if first10 else len(ids)+1):\n",
    "            if ids[i-1] in expectForTheQuery:\n",
    "                relevantDocument.append(relevant/i)\n",
    "                relevant+=1\n",
    "        if len(relevantDocument) != 0:\n",
    "            AP = sum(relevantDocument) / len(relevantDocument)\n",
    "        else:\n",
    "            AP = 0\n",
    "        APs.append(round(AP, 4))\n",
    "        \n",
    "    MAP = sum(APs) / len(APs)\n",
    "    return round(MAP, 4)\n",
    "\n",
    "def tweetIdToString(tweetIdString, tweetPath):\n",
    "    '''\n",
    "    input:34952194402811904,path   output:Save BBC World Service from Savage Cuts http://www.petitionbuzz.com/petitions/savews\n",
    "    '''\n",
    "    tweetIdString = str(tweetIdString)\n",
    "    with open(tweetPath, \"r\") as f:  \n",
    "        for line in f: \n",
    "            line = line.strip('\\ufeff')\n",
    "            if((line.split(\"\\t\"))[0]==tweetIdString):\n",
    "                return (line.split(\"\\t\"))[1][:-1] #[:-1] to remove '\\n'\n",
    "    return\n",
    "\n",
    "def getResultTweet(queryNumInt,result, queries, tweetPath, n=10):\n",
    "    '''\n",
    "    2,result,queries,tweetPath -> first 10 tweet\n",
    "    '''\n",
    "    queryNum = 'B'+(3-len(str(queryNumInt)))*'0'+str(queryNumInt)\n",
    "    print(\"Query %s:\"%(\"number \"+str(queryNumInt)),queries[queryNum],end=\"\\n\\n\")\n",
    "    tweetList = []\n",
    "    tweetIds = result[queryNumInt][:n]\n",
    "    for i in range(1,n+1):\n",
    "        tweetList.append(tweetIdToString(str(tweetIds[i-1]), tweetPath))\n",
    "        print(\"Rank %s: %s\"%(str(i), tweetIdToString(str(tweetIds[i-1]), tweetPath)))\n",
    "    \n",
    "    return tweetList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing line 6000\r"
     ]
    }
   ],
   "source": [
    "token_array = step1_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index, vocabulary, max_tf = step2_indexing(token_array)\n",
    "doc_docno_search_list = [i[0] for i in token_array] # build document-docno search list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetPath = \"./Trec_microblog11.txt\"\n",
    "expectedPath = \"./Trec_microblog11-qrels.txt\"\n",
    "queryPath = \"./topics_MB1-49.txt\"\n",
    "outputPath = \"./output/Result.txt\"\n",
    "queries = loadQueries(queryPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = loadResult(token_array, inverted_index, vocabulary, max_tf, doc_docno_search_list, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expect = loadExpectedResult(expectedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveResults(result,queries,outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAP: %s\"%computeMAP(result,expect))\n",
    "print(\"P@10: %s\"%computeMAP(result,expect,first10=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetResult1 = getResultTweet(3, result, queries, tweetPath)\n",
    "print('\\n\\n%s\\n\\n'%('*'*100))\n",
    "tweetResult2 = getResultTweet(10, result, queries, tweetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_docno_search_list = []\n",
    "# for doc in token_array:\n",
    "#     doc_docno_search_list.append(doc)\n",
    "\n",
    "# # example query token list\n",
    "# example_query_token_list = ['bbc', 'world', 'service', 'staff', 'cuts']\n",
    "\n",
    "\n",
    "# sim_dict = step3_retrieval_and_ranking(token_array, inverted_index,\n",
    "#                                        vocabulary, max_tf, doc_docno_search_list, example_query_token_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
